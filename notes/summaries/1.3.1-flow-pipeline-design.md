# Section 1.3.1: Flow Pipeline Design - Implementation Summary

## Overview

Implemented Flow-based parallel loading pipeline that overlaps dictionary encoding (CPU-bound) with index writing (I/O-bound) for improved bulk loading throughput on multi-core systems.

## Changes Made

### 1. Added Parallel Loading Constants

Added module attributes in `lib/triple_store/loader.ex`:
- `@default_parallel true` - Parallel mode enabled by default
- `@default_max_demand 5` - Default backpressure level
- `@min_stages 1` and `@max_stages 64` - Stage count bounds

### 2. Extended load_opts Type

Updated the `load_opts` type to include parallel loading options:
- `:parallel` - Enable/disable parallel mode (boolean)
- `:stages` - Number of parallel encoding stages (pos_integer)
- `:max_demand` - Backpressure control (pos_integer)

### 3. Implemented load_triples/5 Dispatcher

New dispatcher function that routes to parallel or sequential loading based on options:
```elixir
defp load_triples(db, manager, triples, batch_size, opts) do
  parallel? = Keyword.get(opts, :parallel, @default_parallel)
  if parallel? do
    stages = resolve_stages(opts)
    max_demand = Keyword.get(opts, :max_demand, @default_max_demand)
    load_triples_parallel(db, manager, triples, batch_size, stages, max_demand)
  else
    load_triples_sequential(db, manager, triples, batch_size)
  end
end
```

### 4. Implemented load_triples_parallel/6

Three-stage Flow pipeline:
1. **Chunking**: `Stream.chunk_every/2` divides triples into batches
2. **Parallel encoding**: `Flow.map/2` with multiple stages encodes batches concurrently
3. **Sequential writing**: `Flow.partition(stages: 1)` ensures single-threaded index insertion

Key features:
- Uses Agent for cross-process error tracking
- Proper cleanup with try/after block
- Telemetry events for batch monitoring

### 5. Implemented resolve_stages/1

Configures stage count based on options or system resources:
- Defaults to `System.schedulers_online()` (CPU cores)
- Clamps custom values to valid range (1-64)
- Falls back to CPU cores for invalid inputs

### 6. Implemented Helper Functions

- `encode_batch/3` - Parallel dictionary encoding
- `write_encoded_batch/5` - Sequential index writing with error handling

### 7. Updated Module Documentation

Added parallel loading section to moduledoc with:
- Option descriptions
- Usage examples
- Pipeline architecture explanation

## Files Modified

- `lib/triple_store/loader.ex` - Main implementation
- `test/triple_store/loader/parallel_loading_test.exs` - New test file (26 tests)

## Test Results

- 27 batch size tests passing
- 26 parallel loading tests passing
- 53 total loader tests passing

## Test Coverage

| Category | Tests |
|----------|-------|
| Parallel loading mode | 5 |
| Sequential fallback | 4 |
| Stage count configuration | 6 |
| Backpressure via max_demand | 3 |
| Parallel/sequential consistency | 1 |
| load_stream integration | 4 |
| Edge cases | 3 |

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    Input (Enumerable)                        │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│              Stage 1: Stream.chunk_every                     │
│                    (batch_size)                              │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│    Stage 2: Flow.from_enumerable + Flow.map                 │
│    (stages: N, parallel dictionary encoding)                │
│                                                             │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐        │
│  │ Worker 1│  │ Worker 2│  │ Worker 3│  │ Worker N│        │
│  │ encode  │  │ encode  │  │ encode  │  │ encode  │        │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘        │
│       │            │            │            │              │
└───────┼────────────┼────────────┼────────────┼──────────────┘
        │            │            │            │
        └────────────┴────────────┴────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│    Stage 3: Flow.partition(stages: 1) + Flow.reduce         │
│    (sequential index writing)                               │
│                                                             │
│                   ┌─────────────┐                           │
│                   │  Writer     │                           │
│                   │  (single)   │                           │
│                   └─────────────┘                           │
└───────────────────────────┬─────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────┐
│                    RocksDB Indices                           │
│              (SPO, POS, OSP column families)                │
└─────────────────────────────────────────────────────────────┘
```

## Usage Examples

```elixir
# Default parallel loading (enabled by default)
Loader.load_graph(db, manager, graph)

# Explicit parallel mode
Loader.load_graph(db, manager, graph, parallel: true)

# Sequential mode
Loader.load_graph(db, manager, graph, parallel: false)

# Custom stage count
Loader.load_graph(db, manager, graph, stages: 8)

# Custom backpressure
Loader.load_graph(db, manager, graph, max_demand: 10)

# Combined options
Loader.load_file(db, manager, "data.ttl",
  parallel: true,
  stages: 4,
  max_demand: 5,
  batch_size: 50_000
)
```
