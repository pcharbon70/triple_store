# Task 2.7.4: Performance Benchmarking

## Summary

Created comprehensive performance benchmarks for the SPARQL query engine, measuring execution times for BGP queries, star queries, OPTIONAL patterns, and aggregation operations across various dataset sizes.

## Files Modified

- `test/triple_store/sparql/benchmark_test.exs` - New benchmark test suite (620+ lines)
- `test/test_helper.exs` - Added benchmark exclusion for regular test runs

## Benchmark Suite

### Running Benchmarks

```bash
# Run all benchmarks (small datasets, ~5 min)
mix test test/triple_store/sparql/benchmark_test.exs --include benchmark --exclude large_dataset

# Run all benchmarks including 1M triples (~30+ min)
mix test test/triple_store/sparql/benchmark_test.exs --include benchmark --include large_dataset

# Run specific benchmark
mix test test/triple_store/sparql/benchmark_test.exs:214 --include benchmark
```

### Benchmarks Implemented

#### 2.7.4.1 - Simple BGP Query Benchmark

Tests single-pattern BGP queries across dataset sizes:
- 10K triples: Target <10ms (LIMIT 1000)
- 100K triples: Target <50ms (LIMIT 1000)
- 1M triples: Target <10ms (LIMIT 1000) - ultimate goal

**Sample Results (10K triples):**
```
=== Simple BGP (10K triples, LIMIT 1000) ===
  Min:    7.95ms
  Max:    8.24ms
  Avg:    8.08ms
  Median: 8.07ms
  Results: 1000
  Target: 10ms âœ… PASS
```

#### 2.7.4.2 - Star Query Benchmark (5 patterns)

Tests multi-pattern star queries where all patterns share a common subject:
- 10K entities (50K triples): Target <100ms (LIMIT 100)
- 20K entities (100K triples): Target <200ms (LIMIT 100)
- 200K entities (1M triples): Target <100ms (LIMIT 100)

**Sample Results (200K entities):**
```
=== Star Query 5 patterns (200K entities, LIMIT 100) ===
  Min:    10.03ms
  Max:    10.58ms
  Avg:    10.28ms
  Median: 10.26ms
  Results: 100
  Target: 100ms âœ… PASS
```

#### 2.7.4.3 - OPTIONAL Query Benchmark

Compares OPTIONAL (left join) vs inner join performance:
- 10K entities: Measures overhead percentage
- 100K entities: Measures overhead at scale

**Sample Results (10K entities):**
```
=== Inner Join (10K entities, LIMIT 500) ===
  Median: 26.74ms

=== OPTIONAL (10K entities, LIMIT 500) ===
  Median: 263.97ms

  OPTIONAL overhead: 887.1%
```

**Observations:**
- OPTIONAL queries are significantly slower (8-10x) due to left join processing
- This is expected behavior for left outer join semantics
- Future optimization opportunity: hash-based left join

#### 2.7.4.4 - Aggregation Benchmark

Tests GROUP BY and aggregate function performance:
- COUNT on 5K sales (10K triples): Target <100ms
- SUM on 5K sales (10K triples): Target <200ms
- Implicit grouping (COUNT+SUM+AVG): Target <200ms
- HAVING clause on 50K sales: Target <500ms

**Sample Results:**
```
=== GROUP BY COUNT (5K sales, 100 categories) ===
  Median: 40.49ms
  Target: 100ms âœ… PASS

=== GROUP BY SUM (5K sales, 100 categories) ===
  Median: 68.21ms
  Target: 200ms âœ… PASS
```

## Benchmark Summary Test

A comprehensive summary test runs all benchmark types on the small dataset:

```
============================================================
  SPARQL Query Engine Performance Summary
  Dataset: Small (10K base triples)
============================================================

  Query Type                    Median      Target    Status
  --------------------------------------------------------
  Simple BGP (LIMIT 1000)         ~8ms        10ms      âœ…
  Star Query 5 patterns          ~10ms       100ms      âœ…
  OPTIONAL (LIMIT 500)          ~264ms       100ms      âŒ*
  GROUP BY + SUM                 ~68ms       200ms      âœ…

  * OPTIONAL overhead is expected; documents actual performance
============================================================
```

## Test Infrastructure

### Data Generators

1. **Simple Dataset**: Single predicate pattern for BGP testing
2. **Star Dataset**: 5 properties per entity (name, age, city, category, score)
3. **Optional Dataset**: 50% entities have optional email property
4. **Aggregation Dataset**: Sales with category and amount for GROUP BY testing

### Measurement Utilities

- Warmup runs (3 iterations) before measurement
- Measurement runs (10 iterations) with timing
- Reports min, max, avg, median times
- Automatic PASS/FAIL against targets

## Configuration

Benchmarks are excluded by default via `test_helper.exs`:
```elixir
ExUnit.start(exclude: [:benchmark, :large_dataset])
```

This prevents benchmarks from running during normal test execution, keeping CI fast.

## Performance Targets

| Query Type | Dataset | Target | Status |
|------------|---------|--------|--------|
| Simple BGP | 10K triples | <10ms | âœ… Met |
| Simple BGP | 100K triples | <50ms | âœ… Met |
| Simple BGP | 1M triples | <10ms | â³ Pending* |
| Star Query | 10K entities | <100ms | âœ… Met |
| Star Query | 200K entities | <100ms | âœ… Met |
| OPTIONAL | 10K entities | vs inner | ðŸ“Š Measured |
| Aggregation | 5K sales | <200ms | âœ… Met |

*Large dataset tests require extended runtime

## Key Findings

1. **Simple BGP Performance**: Meets targets with LIMIT, achieving <10ms on 10K triples
2. **Star Query Efficiency**: 5-pattern star queries complete quickly (~10ms for 200K entities)
3. **OPTIONAL Overhead**: Left join semantics introduce 8-10x overhead vs inner join
4. **Aggregation**: GROUP BY with aggregates performs well within targets

## Future Optimization Opportunities

1. **OPTIONAL Performance**: Consider hash-based left join implementation
2. **Aggregation Streaming**: Potential for streaming aggregation with large datasets
3. **Index Utilization**: Verify optimal index selection for star patterns
4. **Memory Profiling**: Add memory usage metrics to benchmarks

## Notes

- Benchmarks use isolated temporary databases
- Each test cleans up resources after completion
- Results may vary based on hardware and system load
- Run benchmarks in isolation for consistent results
